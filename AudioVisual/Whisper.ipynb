{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NbtKmy/gc_workshops/blob/main/Whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9c78y55iwze"
      },
      "source": [
        "# OpenAI-Whisper\n",
        "Whisper ist ein Spracherkennungsmodel, das von OpenAI entwickelt worden ist. \n",
        "Mit Whisper kann man zum Beispiel eine gesprochene Sprache (Audio-Datei) in einen Text umwandeln (transkribieren).\n",
        "\n",
        "Im [Github-Repository](https://github.com/openai/whisper) findet man die Code und Beschreibungen. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEeO6YoM7aEW"
      },
      "source": [
        "## Vor der Installation\n",
        "Vor der Installation überprüfen wir zuerst, ob GPU (Graphicprozessor) für dieses Notebook freigeschaltet ist. Viele KI-Modelle, darunter auch Whisper, braucht GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDT62dUQkLff"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OC3BCeplPQO"
      },
      "source": [
        "Wenn der Befehl oben fehlgeschlagen ist, dann muss man die Einstellung ändern. \n",
        "\"Bearbeiten\"-\"Notebook Einstellungen\" klicken. Dann im geöffneten Popup bei \"Hardwarebeschleuniger\" \"GPU\" anwählen und Speichern.\n",
        "\n",
        "Google bietet schnelle Prozessor wie GPU an, aber nur bis zur gewissen Rechnungskapazität und bis zum gewissen Laufzeit des Prozessors. Wenn man noch mehr Rechnungskapazität will, kann man entweder die Angebote von unterschiedlichen Anbietern beziehen oder Graphic Board selber anschaffen und in dem eigenen PC einbauen.\n",
        "\n",
        "Die [**Science IT**](https://www.zi.uzh.ch/en/teaching-and-research/science-it/computing.html) an der Universität Zürich bietet ebenso die Dienstleistung für solche GPU an."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iityRdIE2SNg"
      },
      "source": [
        "### Version Check von Tensorflow (für Fehlermeldung)\n",
        "\n",
        "In diesem Abschnitt überprüfen wir die Version von Tensorflow. \n",
        "Zuerst den folgenden Befehl laufen lassen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJTnfY390Zmi"
      },
      "outputs": [],
      "source": [
        "!python -c \"import tensorflow as tf; print(tf.__version__)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT-Kbtxj1TRS"
      },
      "source": [
        "Falls jetzt die Fehlermeldung auftaucht, sieht es aus, dass GPU nicht richtig zugegriffen wird:\n",
        "\n",
        "```\n",
        "2023-02-24 09:25:23.842385: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
        "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
        "2023-02-24 09:25:25.054473: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
        "2023-02-24 09:25:25.054576: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
        "2023-02-24 09:25:25.054596: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0QaRXbl59xP"
      },
      "source": [
        ">Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly\n",
        "\n",
        "Das Problem scheint, dass Tensorflow irgendwie Version 7 von libnvinfer.so.7 und libnvinfer_plugin.so.7 suchen geht (und nicht findet). \n",
        "\n",
        "Wenn Tensorflow jetzt GPU zugreifen kann, muss man den folgenden Befehl nicht ausführen.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36gh1e_f0q5x"
      },
      "outputs": [],
      "source": [
        "# wenn die Version von Tersorflow 2.11.0 ist und Fehlermeldung erzeugt wird, dann Tensorflow downgraden\n",
        "# In diesem Prozess ergeben sich einige Errors wegen der Abhängigkeiten der Libraries & Packages.\n",
        "# Aber nach Ausführung des folgenden Befehls taucht das GPU-Probleme nicht mehr auf...\n",
        "\n",
        "!pip install tensorflow-gpu==2.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYDuqlv51zU6"
      },
      "outputs": [],
      "source": [
        "!python -c \"import tensorflow as tf; print(tf.__version__)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5dOVCYQkE49"
      },
      "source": [
        "## Installieren\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd4NlqtIoN-K"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4k4WMCBoTuD"
      },
      "source": [
        "Dann ffmpeg installieren "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR99Foh9oa-k"
      },
      "outputs": [],
      "source": [
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0V_e6Lbo22K"
      },
      "source": [
        "## Einfache Anwendung durch Commandline\n",
        "\n",
        "Zuerst ein Audio-File (.flac, .mp3 oder .wav) hochladen!\n",
        "\n",
        "\n",
        "\n",
        "Bei der Anwendung kann man unterschiedliche Modelle auswählen. \n",
        "\n",
        "Laut der [offziellen Angabe](https://github.com/openai/whisper#available-models-and-languages) gibt es 5 multi-linguale Modelle.\n",
        "\n",
        "Je nach der Sprache ergeben sich unterschiedliche Fehlerquoten. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_3jzBqdeXy1"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "uploaded_filename = list(uploaded.keys())[0]\n",
        "# Unter dem Variabel uploaded_filename ist der Pfad zu dem hochgeladenen File untergebracht"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ma5usGUIHOq2"
      },
      "outputs": [],
      "source": [
        "# Audio anhören\n",
        "from IPython.display import Audio\n",
        "Audio(uploaded_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbOwRcuWqZ1Q"
      },
      "outputs": [],
      "source": [
        "!whisper $uploaded_filename --model medium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2dnVLDu22kQ"
      },
      "outputs": [],
      "source": [
        "!whisper $uploaded_filename --task translate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJGzAJ_B35YI"
      },
      "source": [
        "## In einer Python-Code verwenden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IL_btw3E3-JA"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model('medium')\n",
        "model.device\n",
        "result = model.transcribe(uploaded_filename)\n",
        "print(result['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ujfNP_Dwk4u"
      },
      "source": [
        "Mit der Funktion \"transcribe()\" wird die Audio-Eingabe pro 30 sec. getrennt und Stück für Stück verarbeitet:\n",
        "\n",
        ">Internally, the transcribe() method reads the entire file and processes the audio with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions on each window.\n",
        "(aus: [Python usage](https://github.com/openai/whisper#python-usage))\n",
        "\n",
        "Mit der Funktion \"transcribe()\" kann man deshalb eine Audio-Eingabe länger als 30 sec. am Stück verarbeiten. \n",
        "```\n",
        "model = whisper.load_model([model_size])\n",
        "audio = whisper.load_audio([filename])\n",
        "result = model.transcribe(audio, verbose=True, temperature=0.8, language=[lang])\n",
        "```\n",
        "\n",
        "Wenn man die andere Funktion \"decode()\" verwendet, muss die Länge der Audio-Eingabe max. 30 sec. sein. Wenn die Audio-Aufnahme länger als 30 sec. ist, braucht man ein wenig creativ zu sein. Siehe unten... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4N9nEIevstX"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "\n",
        "fileName = 'result_whisper'#@param {type:'string'}\n",
        "lang = 'de'#@param ['ja', 'en', 'de']\n",
        "model_size = 'medium'#@param ['tiny', 'base', 'small', 'medium', 'large']\n",
        "model = whisper.load_model(model_size)\n",
        "\n",
        "\n",
        "audio = whisper.load_audio(uploaded_filename)\n",
        "outputTextsArr = []\n",
        "\n",
        "while audio.size > 0:\n",
        "    tirmedAudio = whisper.pad_or_trim(audio)\n",
        "    startIdx = tirmedAudio.size\n",
        "    audio = audio[startIdx:]\n",
        "\n",
        "    mel = whisper.log_mel_spectrogram(tirmedAudio).to(model.device)\n",
        "\n",
        "    options = whisper.DecodingOptions(language=lang, without_timestamps=True)\n",
        "    result = whisper.decode(model, mel, options)\n",
        "    outputTextsArr.append(result.text)\n",
        "\n",
        "outputTexts = ' '.join(outputTextsArr)\n",
        "print(outputTexts)\n",
        "\n",
        "\n",
        "with open(f'{fileName}.txt', 'w') as f:\n",
        "    f.write(outputTexts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oytVhf9w6dY_"
      },
      "source": [
        "## Ein YouTube-Video mit Untertitel oder Übersetzung versehen\n",
        "\n",
        "Um ein YouTube-Video herunterzuladen, verwendet man hier [yt-dlp](https://pypi.org/project/yt-dlp/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M23vFQk46msY"
      },
      "outputs": [],
      "source": [
        "!pip install -U yt-dlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y1RuLCn7ZF_"
      },
      "outputs": [],
      "source": [
        "from yt_dlp import YoutubeDL\n",
        "\n",
        "yt_url = 'https://youtu.be/Ifkhj0XtGGM'#@param {type:'string'}\n",
        "vd_filename = 'test'#@param{type:'string'} \n",
        "vd_filename += '.mp4'\n",
        "\n",
        "ydl_opts = {'format': 'best',\n",
        "            'outtmpl': vd_filename }\n",
        "with YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([yt_url])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-mGx3QrA3jB"
      },
      "source": [
        "...Überprüfen, ob das Video tatsächlich heruntergeladen ist..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBu8CqykAOnD"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open(vd_filename, 'rb').read()\n",
        "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"100%\" height=\"100%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spHbeMejeSsC"
      },
      "outputs": [],
      "source": [
        "!pip install -U srt\n",
        "# ffmpeg-python ist bereits mit whisper installiert...\n",
        "# !pip install ffmpeg-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y_hImoOecYM"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "from datetime import timedelta\n",
        "from srt import Subtitle\n",
        "import srt\n",
        "\n",
        "\n",
        "\n",
        "# Text umbrechen, wenn eine Zeile lang ist　... wahrscheinlich braucht man nicht...\n",
        "def add_line(line):\n",
        "    new_line = line\n",
        "    space_count = new_line.count(' ')\n",
        "    list_line = list(new_line)\n",
        "    space_max = 4\n",
        "    if space_count > space_max :\n",
        "        space_index = [i for i, x in enumerate(list_line) if x == ' ']\n",
        "        new_line = line[:space_index[space_max]] + '\\n' + line[space_index[space_max]:]\n",
        "\n",
        "    return new_line\n",
        "\n",
        "# Übersetzung erstellen\n",
        "model = whisper.load_model('medium')\n",
        "result = model.transcribe(vd_filename, task='translate')\n",
        "\n",
        "segments = result['segments']\n",
        " \n",
        "subtitles = []\n",
        " \n",
        "for segment in segments:\n",
        "    index = segment['id'] + 1\n",
        "    start = segment['start']\n",
        "    end = segment['end']\n",
        "    text = add_line(segment['text'])\n",
        "    subtitle = Subtitle(\n",
        "                   index=index, \n",
        "                   start=timedelta(seconds=timedelta(seconds=start).seconds, microseconds=timedelta(seconds=start).microseconds),\n",
        "                   end=timedelta(seconds=timedelta(seconds=end).seconds, microseconds=timedelta(seconds=end).microseconds), \n",
        "                   content=text, \n",
        "                   proprietary=''\n",
        "              )\n",
        "    subtitles.append(subtitle)\n",
        "\n",
        "# Untertitel speichern \n",
        "with open('subtitle.srt', mode='w', encoding='utf-8') as f:\n",
        "    f.write(srt.compose(subtitles))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ffmpeg\n",
        "\n",
        "# Das Video mit Untertitel versehen und speichern\n",
        "video = ffmpeg.input(vd_filename)\n",
        "audio = video.audio\n",
        "ffmpeg.concat(video.filter('subtitles', 'subtitle.srt'), audio, v=1, a=1).output('output_vid.mp4').run()"
      ],
      "metadata": {
        "id": "eaZV7kLv8QxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OcfWgXSH6csp"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open('output_vid.mp4', 'rb').read()\n",
        "vid_wSub = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"100%\" height=\"100%\" controls>\n",
        "      <source src=\"{vid_wSub}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMUipETeQx8rtRVMYcMiHSV",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}