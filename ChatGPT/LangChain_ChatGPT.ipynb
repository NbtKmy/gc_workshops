{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNyMZ/955D9SqWDV8cjGbiz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NbtKmy/gc_workshops/blob/main/LangChain_ChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Was ist LangChain?\n",
        "\n",
        "LangChain ist ein Framework, durch das die Entwicklung einer Applikation mit LLMs (Large Language Models) erleichtert wird. \n",
        "LangChain bieten beispielsweise folgende Module an: \n",
        "\n",
        "| Modul | Funktion |\n",
        "|-------|----------|\n",
        "| Indexes | Indexes wird verwendet, um Texte als Informationsquelle zu einem LLM zu liefern. |\n",
        "| Memory | Memory speichert die Kommunikationshistorie mit dem LLM. |\n",
        "| Agents | Agents bieten Zugriffe auf weitere Informationsquelle durch API. |\n",
        "\n",
        "\n",
        "Durch diese Funktionen kann man eigene Textdaten (zum Beispiel PDF-Dokument oder Text-Datei) oder Wikipedia-Einträge als Informationsquelle für eigenes ChatBot verwenden"
      ],
      "metadata": {
        "id": "YbL2ENuXR1Vk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding mit einem TEI-Daten\n"
      ],
      "metadata": {
        "id": "r1xjBHEcRxVH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uvHD0e20ZnT"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain openai faiss-cpu requests bs4 lxml tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"[your OpenAI-API-Key]\""
      ],
      "metadata": {
        "id": "4OEaoejJIxtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zum Quellentext\n",
        "\n",
        "In diesem Beispiel verwenden wir Kafkas \"Josefine\" aus dem Textgrid: \n",
        " \n",
        " >TextGrid Repository (2012). Kafka, Franz. Josefine, die Sängerin. Digitale Bibliothek. https://hdl.handle.net/11858/00-1734-0000-0003-8DA9-7\n",
        "\n",
        " Diese TEI-Datei ist unter der Lizenz [CC 3.0](https://creativecommons.org/licenses/by/3.0/de/) freigegeben. \n",
        "\n",
        " Die TEI-Datei ist auch [hier](https://textgridlab.org/1.0/tgcrud-public/rest/textgrid:qmv3.0/data) direkt zu nehmen. Aber im folgenden Code laden wir zuerst die TEI-Datei auf unseren eigenen Laptop herunter, dann diese Datei wieder hochladen. "
      ],
      "metadata": {
        "id": "9evBBC0x-yfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "from langchain.schema import Document\n",
        "import re\n",
        "\n",
        "uploaded = files.upload()\n",
        "filename = next(iter(uploaded))\n",
        "\n",
        "data = \"\"\n",
        "with open(filename) as f:\n",
        "    data = f.read()\n",
        "\n",
        "soupObj = BeautifulSoup(data, \"lxml-xml\")\n",
        "\n",
        "titleStmt = soupObj.find(\"titleStmt\")\n",
        "title = titleStmt.find(\"title\").text\n",
        "if soupObj.find(\"notesStmt\"):\n",
        "    notesStmt = soupObj.find(\"notesStmt\")\n",
        "    note = notesStmt.find(\"note\").text\n",
        "else: \n",
        "    note = \"None\"\n",
        "\n",
        "first_pb = soupObj.find(\"pb\")\n",
        "first_n = int(first_pb[\"n\"])\n",
        "paragraphs = soupObj.find(\"text\").find_all(\"p\")\n",
        "pagenum = first_n\n",
        "\n",
        "# Dokument-Objekt von LangChain erstellen und in einem Listen-Objekt hineinschieben\n",
        "def create_document(title, note, pagenum, content, docs):\n",
        "    text_content = content.replace(\"\\n\", \"\")\n",
        "    note = note.replace(\"\\n\", \"\")\n",
        "    note = re.sub(r\"\\s\\s+\", \"\", note)\n",
        "    if len(text_content) > 0:\n",
        "        metadata = {\"source\": title, \"note\": note, \"page\": int(pagenum)}\n",
        "        doc_obj = Document(page_content=content, metadata=metadata)\n",
        "        docs.append(doc_obj)\n",
        "\n",
        "\n",
        "docs = []\n",
        "for paragraph in paragraphs:\n",
        "    \n",
        "    paragraph_contents_list = list(paragraph.children)\n",
        "\n",
        "    if len(paragraph_contents_list) == 3:\n",
        "        pb_index = paragraph.find(\"pb\")\n",
        "        pagenum = int(pb_index[\"n\"])\n",
        "        pagenum_before = pagenum - 1\n",
        "        \n",
        "        before_pb = paragraph_contents_list[:paragraph_contents_list.index(pb_index)][0]\n",
        "        create_document(title, note, pagenum_before, before_pb, docs)\n",
        "\n",
        "        after_pb = paragraph_contents_list[paragraph_contents_list.index(pb_index) + 1:][0]\n",
        "        create_document(title, note, pagenum, after_pb, docs)\n",
        "        \n",
        "    \n",
        "    elif len(paragraph_contents_list) > 3:\n",
        "      \n",
        "        pb_indexes = paragraph.find_all(\"pb\")\n",
        "        \n",
        "        for idx, pb_index in enumerate(pb_indexes): \n",
        "            pagenum = int(pb_index[\"n\"])\n",
        "            sliced_list = paragraph_contents_list[:paragraph_contents_list.index(pb_index)]\n",
        "            sliced_list_after = paragraph_contents_list[paragraph_contents_list.index(pb_index):]\n",
        "            next_pb = pb_indexes[(idx + 1) % len(pb_indexes)]\n",
        "\n",
        "            if len(sliced_list) == 0 or len(sliced_list_after) == 1:\n",
        "                continue\n",
        "\n",
        "            elif len(sliced_list) == 1:\n",
        "                content = sliced_list[0]\n",
        "                create_document(title, note, pagenum - 1, content, docs)\n",
        "                content = paragraph_contents_list[paragraph_contents_list.index(pb_index) +1::paragraph_contents_list.index(next_pb)][0]\n",
        "                create_document(title, note, pagenum, content, docs)\n",
        "\n",
        "            elif len(sliced_list_after) == 2:\n",
        "                content = paragraph_contents_list[paragraph_contents_list.index(pb_index) +1:][0]\n",
        "                create_document(title, note, pagenum, content, docs)\n",
        "            \n",
        "            else:\n",
        "                content = paragraph_contents_list[paragraph_contents_list.index(pb_index) +1::paragraph_contents_list.index(next_pb)][0]\n",
        "                create_document(title, note, pagenum, content, docs)\n",
        "    else: \n",
        "        paragraph_text = paragraph_contents_list[0]\n",
        "        create_document(title, note, pagenum, paragraph_text, docs)\n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "vzxsrlCvJA5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector-Daten in Vector Stores laden\n",
        "\n",
        "Mögliche Vector Stores sind [hier](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html#) zu finden.\n",
        "\n",
        "Hier verwenden wir FAISS.\n",
        "FAISS ist die Abkürzung von \"Facebook AI Similarity Search\". Andere Instanzen wie ElasticSearch oder OpenSearch fordern weitere Installation dieser Instanzen. Um LangChain auszuprobieren, ist FAISS hier ausreichend. Wenn man später eine eigene Anwendung aufbauen will, kommen weitere Instanzen in Frage."
      ],
      "metadata": {
        "id": "-FdgTytV-_jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "query = \"Wie ist der Gesang von Josefine?\"\n",
        "answers = db.similarity_search(query)\n",
        "answers "
      ],
      "metadata": {
        "id": "9I6zOZ_Q-ijo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frage an ChatBot durch LangChain losschicken\n",
        "\n",
        "#### Parameter\n",
        "\n",
        "**tempereture** : Temperature kontrolliert die Kreativität des ChatBots. Es gibt Range zwischen 0 und 2. Je höher der Wert ist, desto kreativer wird die Antwort. \n",
        "Nähere Information ist im [offiziellen Dokument](https://platform.openai.com/docs/api-reference/chat/create#chat/create-temperature) zu finden.\n",
        "\n",
        "**chain_type** : Es sind die 4 Chain-Typen möglich; stuff, map_reduce, refine und map_rerank\n",
        "\n",
        "| Chain-Type | Legende |\n",
        "|-------------|-----------|\n",
        "| stuff | Stuffing is the simplest method, whereby you simply stuff all the related data into the prompt as context to pass to the language model.|\n",
        "| map_reduce | This method involves running an initial prompt on each chunk of data. |\n",
        "| refine | This method involves running an initial prompt on the first chunk of data, generating some output. |\n",
        "| map_rerank | This method involves running an initial prompt on each chunk of data, that not only tries to complete a task but also gives a score for how certain it is in its answer. |\n",
        "\n",
        "[Nähere Beschreibung](https://docs.langchain.com/docs/components/chains/index_related_chains)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "70qzRp1aKB2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(OpenAI(temperature=0.1, max_tokens=1000), chain_type=\"stuff\")\n",
        "query = \"Was ist Josefines Singen eigentlich?\"\n",
        "similar_texts = db.similarity_search(query)\n",
        "# Query mit langChain\n",
        "chain.run(input_documents=similar_texts, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CV4KF2bhJxFS",
        "outputId": "cd6cc0f5-c5d9-48ed-f1fe-580191e0d4d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Es ist nicht klar, ob Josefines Singen eigentlich Gesang oder Pfeifen ist.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wikipedia-Einträge einbinden \n",
        "\n",
        "LangChain bietet [eine Library \"wikipedia\"](https://python.langchain.com/en/latest/modules/agents/tools/examples/wikipedia.html) an. Damit kann man den Inhalt der Wikipedia in Bezug nehmen. \n",
        "\n",
        "Weil wir die Information in Wikipedia für das ChatBot benutzen möchten, wird das Modul \"Agent\" verwendet. \n"
      ],
      "metadata": {
        "id": "ten0D5BSbJIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain openai tiktoken wikipedia"
      ],
      "metadata": {
        "id": "00idU_U_H-7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"Dein API Token hier\""
      ],
      "metadata": {
        "id": "YFF9b_WWIUTY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.llms import OpenAI\n",
        "import wikipedia\n",
        "\n",
        "wikipedia.set_lang(\"de\")\n",
        "\n",
        "llm = OpenAI(temperature=0.2)\n",
        "tools = load_tools([\"wikipedia\"], llm=llm)\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "\n",
        "# Hier wird die Frage formuliert und abgeschickt.\n",
        "agent.run(\"Wann fand Berlinale 2023 statt?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "Jxg58-RGIad_",
        "outputId": "263f9c7b-ea03-4dab-c147-89f06549f092"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out when Berlinale 2023 is happening\n",
            "Action: Wikipedia\n",
            "Action Input: Berlinale 2023\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: 73rd Berlin International Film Festival\n",
            "Summary: The 73rd annual Berlin International Film Festival, usually called the Berlinale (German pronunciation: [bɛʁliˈnaːlə] (listen)), took place from 16 to 26 February 2023. It was the first completely in-person Berlinale since the 70th in 2020. The festival has added a new award for best television series this year. On 15 December 2022, the first Panorama and Generation titles for the festival were announced, and on 13 January 2023, many world premieres were added to out-of-competition lineup, including Israeli filmmaker Guy Nattiv's Golda—a biographical film about Golda Meir, first female Prime Minister of Israel.The festival opened with American filmmaker and novelist Rebecca Miller's drama film She Came to Me. A live video stream with Ukrainian President Volodymyr Zelenskyy was part of the opening ceremony. On 21 February 2023, American filmmaker Steven Spielberg was presented with the Honorary Golden Bear for lifetime achievement by Irish singer-songwriter Bono. Spielberg's films were screened in the Homage section for the occasion. At the award ceremony held on 25 February hosted by Hadnet Tesfai, On the Adamant, a documentary film about a daycare centre in Paris for people with mental disorders, directed by French filmmaker Nicolas Philibert, won the Golden Bear. The Silver Bear Grand Jury Prize was awarded to Afire by German filmmaker Christian Petzold. The highlight of the award ceremony was the Silver Bear for Best Leading Performance won by Sofía Otero for the role of eight-year-old Lucía in 20,000 Species of Bees; Otero, at age nine, became the youngest winner of the award in the Berlinale history.Festival closed on 26 February with total sales of tickets touching 320,000, and around 20,000 accredited professionals from 132 countries including 2,800 media representatives attending the festival.\n",
            "\n",
            "Page: Reality (2023 film)\n",
            "Summary: Reality is an 2023 American drama film, directed by Tina Satter from a screenplay by Satter and James Paul Dallas, adapted from the FBI interrogation transcript of American intelligence leaker Reality Winner, which Satter previously staged as the play Is This A Room. It stars Sydney Sweeney as Winner, with Marchánt Davis and Josh Hamilton in supporting roles.\n",
            "Reality premiered at the 2023 Berlin International Film Festival on February 18, 2023. It is scheduled to be released on May 29, 2023, by HBO Films.\n",
            "\n",
            "Page: The Echo (2023 film)\n",
            "Summary: The Echo (Spanish: El eco) is a 2023 Mexican-German documentary film directed by Tatiana Huezo. The film, which also includes fictional elements, portrays the children of an isolated village \"El Echo\", in the Mexican highlands. It is selected in Encounter at the 73rd Berlin International Film Festival, where it had its world premiere on 17 February 2023. The film is also nominated for Berlinale Documentary Film Award.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Die 73. Berlinale fand vom 16. bis 26. Februar 2023 statt.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Die 73. Berlinale fand vom 16. bis 26. Februar 2023 statt.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}